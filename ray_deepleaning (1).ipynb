{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d17a1dc-1a7f-490d-a30d-e8988d77a8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 01:05:52,477\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-12-04 01:05:57,038\tINFO worker.py:1634 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379...\n",
      "2024-12-04 01:05:57,059\tINFO worker.py:1819 -- Connected to Ray cluster.\n",
      "\u001b[36m(pid=78280)\u001b[0m 2024-12-04 01:05:58.952490: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=78280)\u001b[0m 2024-12-04 01:06:00.031805: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(train_worker pid=78280)\u001b[0m C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\u001b[36m(train_worker pid=78280)\u001b[0m   super().__init__(**kwargs)\n",
      "\u001b[36m(train_worker pid=78280)\u001b[0m 2024-12-04 01:06:02.356063: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(train_worker pid=78280)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_worker pid=78280)\u001b[0m Worker started with data shape: (30000, 28, 28)\n",
      "\u001b[36m(train_worker pid=78280)\u001b[0m Worker started with data shape: (30000, 28, 28)\n",
      "Scalability Test: 2 Workers\n",
      "Average Accuracy: 0.9514\n",
      "Training Time: 14.12 seconds\n",
      "\u001b[36m(train_worker pid=78280)\u001b[0m Worker started with data shape: (15000, 28, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=79464)\u001b[0m 2024-12-04 01:06:12.139545: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=77844)\u001b[0m 2024-12-04 01:06:13.106679: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_worker pid=78280)\u001b[0m Worker started with data shape: (15000, 28, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_worker pid=77844)\u001b[0m C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\u001b[36m(train_worker pid=77844)\u001b[0m   super().__init__(**kwargs)\n",
      "\u001b[36m(train_worker pid=77844)\u001b[0m 2024-12-04 01:06:17.750466: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(train_worker pid=77844)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=77844)\u001b[0m 2024-12-04 01:06:14.611438: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalability Test: 4 Workers\n",
      "Average Accuracy: 0.9375\n",
      "Training Time: 11.61 seconds\n",
      "\u001b[36m(train_worker pid=78280)\u001b[0m Worker started with data shape: (10000, 28, 28)\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_worker pid=79464)\u001b[0m C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\u001b[36m(train_worker pid=79464)\u001b[0m   super().__init__(**kwargs)\n",
      "\u001b[36m(train_worker pid=79464)\u001b[0m 2024-12-04 01:06:18.218980: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(train_worker pid=79464)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=73708)\u001b[0m 2024-12-04 01:06:24.132123: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=73180)\u001b[0m 2024-12-04 01:06:25.948111: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_worker pid=77844)\u001b[0m Worker started with data shape: (10000, 28, 28)\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_worker pid=73708)\u001b[0m C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\u001b[36m(train_worker pid=73708)\u001b[0m   super().__init__(**kwargs)\n",
      "\u001b[36m(train_worker pid=73708)\u001b[0m 2024-12-04 01:06:33.411450: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(train_worker pid=73708)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=73180)\u001b[0m 2024-12-04 01:06:28.959555: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalability Test: 6 Workers\n",
      "Average Accuracy: 0.9280\n",
      "Training Time: 13.37 seconds\n",
      "\u001b[36m(train_worker pid=73708)\u001b[0m Worker started with data shape: (7500, 28, 28)\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_worker pid=73180)\u001b[0m C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\u001b[36m(train_worker pid=73180)\u001b[0m   super().__init__(**kwargs)\n",
      "\u001b[36m(train_worker pid=73180)\u001b[0m 2024-12-04 01:06:33.369454: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(train_worker pid=73180)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=71572)\u001b[0m 2024-12-04 01:06:42.334933: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_worker pid=71572)\u001b[0m C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\u001b[36m(train_worker pid=71572)\u001b[0m   super().__init__(**kwargs)\n",
      "\u001b[36m(train_worker pid=71572)\u001b[0m 2024-12-04 01:06:48.119596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(train_worker pid=71572)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_worker pid=71572)\u001b[0m Worker started with data shape: (7500, 28, 28)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Scalability Test: 8 Workers\n",
      "Average Accuracy: 0.9197\n",
      "Training Time: 14.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=77356)\u001b[0m 2024-12-04 01:06:52.200519: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_worker pid=71572)\u001b[0m Worker started with data shape: (6666, 28, 28)\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=77356)\u001b[0m 2024-12-04 01:06:56.557265: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(train_worker pid=77356)\u001b[0m C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\u001b[36m(train_worker pid=77356)\u001b[0m   super().__init__(**kwargs)\n",
      "\u001b[36m(train_worker pid=77356)\u001b[0m 2024-12-04 01:07:03.291065: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(train_worker pid=77356)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_worker pid=77356)\u001b[0m Worker started with data shape: (6666, 28, 28)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Scalability Test: 9 Workers\n",
      "Average Accuracy: 0.9181\n",
      "Training Time: 15.46 seconds\n",
      "\u001b[36m(train_worker pid=78280)\u001b[0m Worker started with data shape: (5000, 28, 28)\n",
      "\u001b[36m(train_worker pid=79464)\u001b[0m Worker started with data shape: (5000, 28, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=79284)\u001b[0m 2024-12-04 01:07:08.707559: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=79284)\u001b[0m 2024-12-04 01:07:13.252959: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(train_worker pid=79284)\u001b[0m C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\u001b[36m(train_worker pid=79284)\u001b[0m   super().__init__(**kwargs)\n",
      "\u001b[36m(train_worker pid=79284)\u001b[0m 2024-12-04 01:07:20.283185: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(train_worker pid=79284)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=81480)\u001b[0m 2024-12-04 01:07:16.191959: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_worker pid=79284)\u001b[0m Worker started with data shape: (5000, 28, 28)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Scalability Test: 12 Workers\n",
      "Average Accuracy: 0.9061\n",
      "Training Time: 17.54 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 01:07:24,330\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::faulty_worker()\u001b[39m (pid=77844, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1862, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\PMLS\\AppData\\Local\\Temp\\ipykernel_66768\\227194966.py\", line 83, in faulty_worker\n",
      "Exception: Simulated worker failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault Tolerance Test: Worker with PID 66768 failed. Error: \u001b[36mray::faulty_worker()\u001b[39m (pid=77844, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1862, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\PMLS\\AppData\\Local\\Temp\\ipykernel_66768\\227194966.py\", line 83, in faulty_worker\n",
      "Exception: Simulated worker failure\n",
      "Total workers: 5, Failed workers: 0, Successful workers: 0\n",
      "Resource Usage:\n",
      "Total CPU Cores: 12\n",
      "Available CPU Cores: 10\n",
      "Total Memory: 7.73 GB\n",
      "Used Memory: 7.69 GB\n",
      "\u001b[36m(train_worker pid=81480)\u001b[0m Worker started with data shape: (5000, 28, 28)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_worker pid=81480)\u001b[0m C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_worker pid=81480)\u001b[0m   super().__init__(**kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_worker pid=81480)\u001b[0m 2024-12-04 01:07:21.080023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_worker pid=81480)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# TensorFlow threading optimization\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init()\n",
    "\n",
    "# Load and preprocess MNIST data\n",
    "def preprocess_mnist():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# More complex model with Dropout layer\n",
    "def build_complex_model():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Ray Remote Training Worker\n",
    "@ray.remote(num_cpus=1)  # Explicitly allocate 1 CPU per worker\n",
    "def train_worker(x_data, y_data, epochs=1):\n",
    "    print(f\"Worker started with data shape: {x_data.shape}\")\n",
    "    model = build_complex_model()\n",
    "    model.fit(x_data, y_data, epochs=epochs, verbose=0)\n",
    "    return model.evaluate(x_data, y_data, verbose=0)\n",
    "\n",
    "# Measure scalability and resource usage\n",
    "def measure_scalability(num_workers, x_train, y_train):\n",
    "    split_data = np.array_split(x_train, num_workers)\n",
    "    split_labels = np.array_split(y_train, num_workers)\n",
    "\n",
    "    workers = []\n",
    "    start_time = time.time()\n",
    "    for i in range(num_workers):\n",
    "        workers.append(train_worker.remote(split_data[i], split_labels[i]))\n",
    "\n",
    "    results = ray.get(workers)\n",
    "    end_time = time.time()\n",
    "\n",
    "    avg_accuracy = np.mean([result[1] for result in results])\n",
    "    print(f\"Scalability Test: {num_workers} Workers\")\n",
    "    print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Monitor resource usage\n",
    "def monitor_resource_usage():\n",
    "    total_cpus = psutil.cpu_count(logical=True)  # Total CPU cores (including logical CPUs)\n",
    "    available_cpus = psutil.cpu_count(logical=False)  # Physical cores (if Hyperthreading is on, it will be less than total logical CPUs)\n",
    "    total_memory = psutil.virtual_memory().total / (1024 ** 3)  # Total memory in GB\n",
    "    used_memory = psutil.virtual_memory().used / (1024 ** 3)  # Used memory in GB\n",
    "\n",
    "    print(f\"Resource Usage:\")\n",
    "    print(f\"Total CPU Cores: {total_cpus}\")\n",
    "    print(f\"Available CPU Cores: {available_cpus}\")\n",
    "    print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Used Memory: {used_memory:.2f} GB\")\n",
    "\n",
    "# Test fault tolerance\n",
    "@ray.remote(num_cpus=1)\n",
    "def faulty_worker(x_data, y_data):\n",
    "    if np.random.rand() < 0.3:  # Simulate random failure\n",
    "        raise Exception(\"Simulated worker failure\")\n",
    "    model = build_complex_model()\n",
    "    model.fit(x_data, y_data, epochs=1, verbose=0)\n",
    "    return model.evaluate(x_data, y_data, verbose=0)\n",
    "\n",
    "# Modified Fault Tolerance Test with Average Accuracy Calculation\n",
    "def test_fault_tolerance(x_train, y_train):\n",
    "    num_workers = 5\n",
    "    split_data = np.array_split(x_train, num_workers)\n",
    "    split_labels = np.array_split(y_train, num_workers)\n",
    "\n",
    "    workers = []\n",
    "    for i in range(num_workers):\n",
    "        workers.append(faulty_worker.remote(split_data[i], split_labels[i]))\n",
    "\n",
    "    successful_results = []\n",
    "    failed_workers = 0\n",
    "\n",
    "    try:\n",
    "        results = ray.get(workers)\n",
    "        for result in results:\n",
    "            if result is not None:\n",
    "                successful_results.append(result[1])\n",
    "            else:\n",
    "                failed_workers += 1\n",
    "        \n",
    "        if successful_results:\n",
    "            avg_accuracy = np.mean(successful_results)\n",
    "            print(f\"Fault Tolerance Test Passed. Avg Accuracy (successful workers): {avg_accuracy:.4f}\")\n",
    "        else:\n",
    "            print(\"No successful workers in fault tolerance test.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Mask system path, only show the pid and exception message\n",
    "        pid = os.getpid()  # Get the current process ID\n",
    "        print(f\"Fault Tolerance Test: Worker with PID {pid} failed. Error: {str(e)}\")\n",
    "        \n",
    "    print(f\"Total workers: {num_workers}, Failed workers: {failed_workers}, Successful workers: {len(successful_results)}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    x_train, y_train, x_test, y_test = preprocess_mnist()\n",
    "\n",
    "    # Test scalability\n",
    "    for workers in [2, 4, 6, 8, 9, 12]:  # Test with different numbers of workers\n",
    "        measure_scalability(workers, x_train, y_train)\n",
    "\n",
    "    # Test fault tolerance with average accuracy calculation\n",
    "    test_fault_tolerance(x_train, y_train)\n",
    "\n",
    "    # Monitor resource usage\n",
    "    monitor_resource_usage()\n",
    "\n",
    "    # Shutdown Ray\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325307c7-6052-48e1-a295-da9df271dbad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
